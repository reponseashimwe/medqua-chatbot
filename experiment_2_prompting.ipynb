{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ§ª Experiment 2: Prompting Strategies\n",
        "\n",
        "## ðŸŽ¯ Hypothesis\n",
        "The \"improved\" model failed because the prompt was too generic and encouraged template-like responses. Different prompts might improve factual accuracy.\n",
        "\n",
        "## ðŸ”¬ Changes from Baseline:\n",
        "- âœ… Test 3 different prompting approaches:\n",
        "  1. **Extractive**: \"summarize: \" (encourages extracting key facts)\n",
        "  2. **Concise**: \"medical answer: \" (encourages direct medical answers)\n",
        "  3. **Context-aware**: \"question: {Q} context: medical knowledge answer: \" (structured approach)\n",
        "- âœ… Same T5-Small model (fair comparison)\n",
        "- âœ… Same optimized hyperparameters as Experiment 1\n",
        "- âœ… Sequence lengths: 256/512 (restored)\n",
        "\n",
        "**Model:** T5-Small (77M params)\n",
        "\n",
        "**Baseline to Beat:**\n",
        "- BLEU: 0.0283\n",
        "- ROUGE-L: 0.2102\n",
        "\n",
        "**Note:** Run all 3 prompt variations, then compare results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q tf-keras transformers tensorflow pandas numpy scikit-learn datasets evaluate rouge-score sacrebleu matplotlib seaborn\n",
        "print(\"âœ“ Installed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "from pathlib import Path\n",
        "from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM\n",
        "from datasets import Dataset\n",
        "import evaluate\n",
        "import tensorflow as tf\n",
        "import tf_keras\n",
        "from datetime import datetime\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "print(f\"TensorFlow: {tf.__version__}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load preprocessed data\n",
        "train_df = pd.read_csv('data/improved/train_improved.csv')\n",
        "val_df = pd.read_csv('data/improved/val_improved.csv')\n",
        "test_df = pd.read_csv('data/improved/test_improved.csv')\n",
        "print(f\"Train: {len(train_df):,} | Val: {len(val_df):,} | Test: {len(test_df):,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”¬ KEY DIFFERENCE: Prompting Strategies\n",
        "\n",
        "Choose ONE prompt strategy below and run the experiment:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CHOOSE PROMPT STRATEGY (uncomment ONE):\n",
        "# PROMPT_STRATEGY = \"extractive\"   # Option 1: Extractive style\n",
        "PROMPT_STRATEGY = \"concise\"      # Option 2: Concise medical answer (RECOMMENDED)\n",
        "# PROMPT_STRATEGY = \"contextual\"  # Option 3: Context-aware structured\n",
        "\n",
        "PROMPT_TEMPLATES = {\n",
        "    \"extractive\": \"summarize: \",\n",
        "    \"concise\": \"medical answer: \",\n",
        "    \"contextual\": \"question: {question} context: medical knowledge answer: \"\n",
        "}\n",
        "\n",
        "print(f\"ðŸŽ¯ TESTING PROMPT STRATEGY: {PROMPT_STRATEGY}\")\n",
        "print(f\"Template: '{PROMPT_TEMPLATES[PROMPT_STRATEGY]}'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MODEL_NAME = \"t5-small\"\n",
        "MAX_INPUT_LENGTH = 256\n",
        "MAX_TARGET_LENGTH = 512\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "print(f\"âœ“ {MODEL_NAME} loaded | Vocab: {len(tokenizer):,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "    \"\"\"Tokenization with selected prompt strategy.\"\"\"\n",
        "    if PROMPT_STRATEGY == \"contextual\":\n",
        "        inputs = [PROMPT_TEMPLATES[PROMPT_STRATEGY].format(question=q) for q in examples['question']]\n",
        "    else:\n",
        "        inputs = [PROMPT_TEMPLATES[PROMPT_STRATEGY] + q for q in examples['question']]\n",
        "    \n",
        "    targets = examples['answer']\n",
        "    \n",
        "    model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LENGTH, truncation=True, padding='max_length')\n",
        "    labels = tokenizer(targets, max_length=MAX_TARGET_LENGTH, truncation=True, padding='max_length')\n",
        "    \n",
        "    model_inputs['labels'] = labels['input_ids']\n",
        "    return model_inputs\n",
        "\n",
        "print(\"âœ“ Preprocessing function ready with\", PROMPT_STRATEGY, \"strategy\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_ds = Dataset.from_pandas(train_df)\n",
        "val_ds = Dataset.from_pandas(val_df)\n",
        "test_ds = Dataset.from_pandas(test_df)\n",
        "\n",
        "tok_train = train_ds.map(preprocess_function, batched=True, remove_columns=train_ds.column_names)\n",
        "tok_val = val_ds.map(preprocess_function, batched=True, remove_columns=val_ds.column_names)\n",
        "tok_test = test_ds.map(preprocess_function, batched=True, remove_columns=test_ds.column_names)\n",
        "\n",
        "print(f\"âœ“ Tokenized: {len(tok_train):,} train samples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = TFAutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
        "print(f\"âœ“ Model: {model.num_parameters():,} params\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Same optimized hyperparameters as Experiment 1\n",
        "BATCH_SIZE = 8\n",
        "GRADIENT_ACCUM = 2\n",
        "LEARNING_RATE = 5e-5\n",
        "EPOCHS = 8\n",
        "WEIGHT_DECAY = 0.01\n",
        "WARMUP_RATIO = 0.1\n",
        "\n",
        "print(f\"Hyperparameters: LR={LEARNING_RATE}, Epochs={EPOCHS}, Batch={BATCH_SIZE}\")\n",
        "\n",
        "tf_train = model.prepare_tf_dataset(tok_train, batch_size=BATCH_SIZE, shuffle=True, tokenizer=tokenizer)\n",
        "tf_val = model.prepare_tf_dataset(tok_val, batch_size=BATCH_SIZE, shuffle=False, tokenizer=tokenizer)\n",
        "\n",
        "# Setup optimizer (same as Experiment 1)\n",
        "num_steps = (len(tf_train) // GRADIENT_ACCUM) * EPOCHS\n",
        "num_warmup = int(WARMUP_RATIO * num_steps)\n",
        "\n",
        "lr_schedule = tf_keras.optimizers.schedules.PolynomialDecay(\n",
        "    initial_learning_rate=LEARNING_RATE, decay_steps=num_steps - num_warmup, end_learning_rate=1e-7\n",
        ")\n",
        "\n",
        "class WarmupSchedule(tf_keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, warmup_steps, post_warmup_schedule):\n",
        "        super().__init__()\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.post_warmup_schedule = post_warmup_schedule\n",
        "        self.learning_rate = LEARNING_RATE\n",
        "        \n",
        "    def __call__(self, step):\n",
        "        step = tf.cast(step, tf.float32)\n",
        "        warmup_steps = tf.cast(self.warmup_steps, tf.float32)\n",
        "        warmup_lr = self.learning_rate * (step / warmup_steps)\n",
        "        decay_lr = self.post_warmup_schedule(step - warmup_steps)\n",
        "        return tf.cond(step < warmup_steps, lambda: warmup_lr, lambda: decay_lr)\n",
        "\n",
        "final_schedule = WarmupSchedule(num_warmup, lr_schedule)\n",
        "optimizer = tf_keras.optimizers.AdamW(learning_rate=final_schedule, weight_decay=WEIGHT_DECAY)\n",
        "model.compile(optimizer=optimizer)\n",
        "\n",
        "print(\"âœ“ Optimizer ready\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tf_keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "exp_dir = f\"models/experiment_2_{PROMPT_STRATEGY}\"\n",
        "Path(exp_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1),\n",
        "    ModelCheckpoint(f'{exp_dir}/best.h5', monitor='val_loss', save_best_only=True, save_weights_only=True, verbose=1)\n",
        "]\n",
        "\n",
        "print(f\"âœ“ Will save to {exp_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(f\"ðŸš€ EXPERIMENT 2: PROMPTING STRATEGY = {PROMPT_STRATEGY.upper()}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "start_time = datetime.now()\n",
        "\n",
        "history = model.fit(tf_train, validation_data=tf_val, epochs=EPOCHS, callbacks=callbacks, verbose=1)\n",
        "\n",
        "training_time = (datetime.now() - start_time).total_seconds()\n",
        "\n",
        "print(f\"\\\\nâœ“ Training complete in {training_time/60:.1f} minutes\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(history.history['loss'], 'o-', label='Train', lw=2)\n",
        "plt.plot(history.history['val_loss'], 's-', label='Val', lw=2)\n",
        "plt.xlabel('Epoch'); plt.ylabel('Loss')\n",
        "plt.title(f'Exp 2: {PROMPT_STRATEGY} prompting')\n",
        "plt.legend(); plt.grid(True, alpha=0.3)\n",
        "plt.savefig(f'data/improved/experiment_2_{PROMPT_STRATEGY}.png', dpi=300)\n",
        "plt.show()\n",
        "\n",
        "print(f\"Best Val Loss: {min(history.history['val_loss']):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“Š Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_answer(question, model, tokenizer):\n",
        "    if PROMPT_STRATEGY == \"contextual\":\n",
        "        prefix = PROMPT_TEMPLATES[PROMPT_STRATEGY].format(question=question)\n",
        "    else:\n",
        "        prefix = PROMPT_TEMPLATES[PROMPT_STRATEGY] + question\n",
        "    \n",
        "    inputs = tokenizer(prefix, return_tensors='tf', max_length=MAX_INPUT_LENGTH, truncation=True)\n",
        "    outputs = model.generate(**inputs, max_length=512, min_length=10, num_beams=4, early_stopping=True,\n",
        "                            no_repeat_ngram_size=3, length_penalty=1.0, do_sample=False)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "bleu_metric = evaluate.load(\"bleu\")\n",
        "rouge_metric = evaluate.load(\"rouge\")\n",
        "\n",
        "print(\"Generating 200 predictions...\")\n",
        "predictions, references = [], []\n",
        "\n",
        "for i in range(min(200, len(test_df))):\n",
        "    q = test_df.iloc[i]['question']\n",
        "    predictions.append(generate_answer(q, model, tokenizer))\n",
        "    references.append(test_df.iloc[i]['answer'])\n",
        "    if (i+1) % 50 == 0:\n",
        "        print(f\"  {i+1}/200...\")\n",
        "\n",
        "print(\"âœ“ Done\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bleu_result = bleu_metric.compute(predictions=predictions, references=[[r] for r in references])\n",
        "rouge_result = rouge_metric.compute(predictions=predictions, references=references)\n",
        "val_loss = min(history.history['val_loss'])\n",
        "\n",
        "print(\"\\\\n\" + \"=\"*80)\n",
        "print(f\"ðŸ“Š EXPERIMENT 2 RESULTS ({PROMPT_STRATEGY})\")\n",
        "print(\"=\"*80)\n",
        "print(f\"BLEU:     {bleu_result['bleu']:.4f}\")\n",
        "print(f\"ROUGE-L:  {rouge_result['rougeL']:.4f}\")\n",
        "print(f\"Val Loss: {val_loss:.4f}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Save results\n",
        "results = {\n",
        "    'experiment_id': f'exp2_{PROMPT_STRATEGY}',\n",
        "    'experiment_name': f'Prompting Strategy: {PROMPT_STRATEGY}',\n",
        "    'model_name': MODEL_NAME,\n",
        "    'prompt_strategy': PROMPT_STRATEGY,\n",
        "    'prompt_template': PROMPT_TEMPLATES[PROMPT_STRATEGY],\n",
        "    'hyperparameters': {\n",
        "        'batch_size': BATCH_SIZE,\n",
        "        'learning_rate': LEARNING_RATE,\n",
        "        'epochs': EPOCHS,\n",
        "        'max_input_length': MAX_INPUT_LENGTH,\n",
        "        'max_target_length': MAX_TARGET_LENGTH\n",
        "    },\n",
        "    'metrics': {\n",
        "        'bleu': float(bleu_result['bleu']),\n",
        "        'rougeL': float(rouge_result['rougeL']),\n",
        "        'val_loss': float(val_loss)\n",
        "    },\n",
        "    'training_time_seconds': training_time\n",
        "}\n",
        "\n",
        "Path('results').mkdir(exist_ok=True)\n",
        "with open(f'results/experiment_2_{PROMPT_STRATEGY}.json', 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(f\"\\\\nâœ“ Results saved to results/experiment_2_{PROMPT_STRATEGY}.json\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
