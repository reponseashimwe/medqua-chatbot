{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ§ª Experiment 4: Lower Learning Rate + Longer Training\n",
        "\n",
        "## ðŸŽ¯ Hypothesis\n",
        "The baseline might have been undertrained. A lower learning rate with more epochs and patience could find a better minimum.\n",
        "\n",
        "## ðŸ”¬ Changes:\n",
        "- âœ… Model: T5-Small (same as baseline for fair comparison)\n",
        "- âœ… **Very low LR: 2e-5** (vs 5e-5 in Exp 1)\n",
        "- âœ… More epochs: 10 (vs 8 in Exp 1)\n",
        "- âœ… More patience: 4 (vs 3)\n",
        "- âœ… Sequence lengths: 256/512 (restored)\n",
        "\n",
        "**Strategy:** Train slower but longer, let the model converge better\n",
        "\n",
        "**Baseline to Beat:**\n",
        "- BLEU: 0.0283\n",
        "- ROUGE-L: 0.2102\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q tf-keras transformers tensorflow pandas numpy scikit-learn datasets evaluate rouge-score sacrebleu matplotlib seaborn\n",
        "import pandas as pd, numpy as np, matplotlib.pyplot as plt, json\n",
        "from pathlib import Path\n",
        "from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM\n",
        "from datasets import Dataset\n",
        "import evaluate, tensorflow as tf, tf_keras\n",
        "from datetime import datetime\n",
        "\n",
        "SEED = 42; np.random.seed(SEED); tf.random.set_seed(SEED)\n",
        "print(\"âœ“ Setup complete\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df = pd.read_csv('data/improved/train_improved.csv')\n",
        "val_df = pd.read_csv('data/improved/val_improved.csv')\n",
        "test_df = pd.read_csv('data/improved/test_improved.csv')\n",
        "\n",
        "MODEL_NAME = \"t5-small\"\n",
        "MAX_INPUT_LENGTH, MAX_TARGET_LENGTH = 256, 512\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [\"answer: \" + q for q in examples['question']]\n",
        "    model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LENGTH, truncation=True, padding='max_length')\n",
        "    labels = tokenizer(examples['answer'], max_length=MAX_TARGET_LENGTH, truncation=True, padding='max_length')\n",
        "    model_inputs['labels'] = labels['input_ids']\n",
        "    return model_inputs\n",
        "\n",
        "train_ds, val_ds, test_ds = Dataset.from_pandas(train_df), Dataset.from_pandas(val_df), Dataset.from_pandas(test_df)\n",
        "tok_train = train_ds.map(preprocess_function, batched=True, remove_columns=train_ds.column_names)\n",
        "tok_val = val_ds.map(preprocess_function, batched=True, remove_columns=val_ds.column_names)\n",
        "\n",
        "print(f\"âœ“ Data ready: {len(tok_train):,} train\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = TFAutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# LOWER LR, MORE EPOCHS\n",
        "BATCH_SIZE = 8\n",
        "LEARNING_RATE = 2e-5  # LOWER!\n",
        "EPOCHS = 10           # MORE!\n",
        "WEIGHT_DECAY = 0.01\n",
        "WARMUP_RATIO = 0.1\n",
        "\n",
        "print(f\"ðŸŽ¯ KEY DIFFERENCE: LR={LEARNING_RATE} (lower) | Epochs={EPOCHS} (more)\")\n",
        "\n",
        "tf_train = model.prepare_tf_dataset(tok_train, batch_size=BATCH_SIZE, shuffle=True, tokenizer=tokenizer)\n",
        "tf_val = model.prepare_tf_dataset(tok_val, batch_size=BATCH_SIZE, shuffle=False, tokenizer=tokenizer)\n",
        "\n",
        "num_steps = len(tf_train) * EPOCHS\n",
        "num_warmup = int(WARMUP_RATIO * num_steps)\n",
        "\n",
        "class WarmupSchedule(tf_keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, lr, warmup_steps, total_steps):\n",
        "        super().__init__(); self.lr, self.warmup_steps, self.total_steps = lr, warmup_steps, total_steps\n",
        "    def __call__(self, step):\n",
        "        step = tf.cast(step, tf.float32)\n",
        "        warmup = self.lr * (step / tf.cast(self.warmup_steps, tf.float32))\n",
        "        decay_steps = self.total_steps - self.warmup_steps\n",
        "        decay = self.lr * (1 - (step - self.warmup_steps) / decay_steps)\n",
        "        return tf.cond(step < self.warmup_steps, lambda: warmup, lambda: tf.maximum(decay, 1e-7))\n",
        "\n",
        "optimizer = tf_keras.optimizers.AdamW(learning_rate=WarmupSchedule(LEARNING_RATE, num_warmup, num_steps), weight_decay=WEIGHT_DECAY)\n",
        "model.compile(optimizer=optimizer)\n",
        "print(\"âœ“ Optimizer ready\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tf_keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "Path('models/experiment_4').mkdir(parents=True, exist_ok=True)\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True, verbose=1),  # More patience!\n",
        "    ModelCheckpoint('models/experiment_4/best.h5', monitor='val_loss', save_best_only=True, save_weights_only=True, verbose=1)\n",
        "]\n",
        "\n",
        "print(\"ðŸš€ EXPERIMENT 4: LOWER LR + LONGER TRAINING\")\n",
        "start_time = datetime.now()\n",
        "history = model.fit(tf_train, validation_data=tf_val, epochs=EPOCHS, callbacks=callbacks, verbose=1)\n",
        "training_time = (datetime.now() - start_time).total_seconds()\n",
        "print(f\"âœ“ Complete in {training_time/60:.1f} min\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(history.history['loss'], 'o-', label='Train', lw=2)\n",
        "plt.plot(history.history['val_loss'], 's-', label='Val', lw=2)\n",
        "plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title('Exp 4: Lower LR')\n",
        "plt.legend(); plt.grid(True, alpha=0.3)\n",
        "plt.savefig('data/improved/experiment_4_training.png', dpi=300)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluation\n",
        "def generate_answer(q, model, tok):\n",
        "    inp = tok(\"answer: \" + q, return_tensors='tf', max_length=MAX_INPUT_LENGTH, truncation=True)\n",
        "    out = model.generate(**inp, max_length=512, num_beams=4, early_stopping=True, no_repeat_ngram_size=3, do_sample=False)\n",
        "    return tok.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "bleu_metric, rouge_metric = evaluate.load(\"bleu\"), evaluate.load(\"rouge\")\n",
        "predictions, references = [], []\n",
        "\n",
        "for i in range(min(200, len(test_df))):\n",
        "    predictions.append(generate_answer(test_df.iloc[i]['question'], model, tokenizer))\n",
        "    references.append(test_df.iloc[i]['answer'])\n",
        "    if (i+1) % 50 == 0: print(f\"{i+1}/200...\")\n",
        "\n",
        "bleu_result = bleu_metric.compute(predictions=predictions, references=[[r] for r in references])\n",
        "rouge_result = rouge_metric.compute(predictions=predictions, references=references)\n",
        "val_loss = min(history.history['val_loss'])\n",
        "\n",
        "print(\"\\\\n\" + \"=\"*80)\n",
        "print(\"ðŸ“Š EXPERIMENT 4 RESULTS\")\n",
        "print(\"=\"*80)\n",
        "print(f\"BLEU:     {bleu_result['bleu']:.4f}\")\n",
        "print(f\"ROUGE-L:  {rouge_result['rougeL']:.4f}\")\n",
        "print(f\"Val Loss: {val_loss:.4f}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "results = {\n",
        "    'experiment_id': 'exp4_lower_lr',\n",
        "    'experiment_name': 'Lower LR + Longer Training',\n",
        "    'model_name': MODEL_NAME,\n",
        "    'hyperparameters': {'batch_size': BATCH_SIZE, 'learning_rate': LEARNING_RATE, 'epochs': EPOCHS,\n",
        "                       'max_input_length': MAX_INPUT_LENGTH, 'max_target_length': MAX_TARGET_LENGTH},\n",
        "    'metrics': {'bleu': float(bleu_result['bleu']), 'rougeL': float(rouge_result['rougeL']), 'val_loss': float(val_loss)},\n",
        "    'training_time_seconds': training_time\n",
        "}\n",
        "\n",
        "Path('results').mkdir(exist_ok=True)\n",
        "with open('results/experiment_4_results.json', 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "print(\"âœ“ Saved to results/experiment_4_results.json\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
